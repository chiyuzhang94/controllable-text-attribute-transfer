{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Import your model files.\n",
    "from model import make_model, Classifier, NoamOpt, LabelSmoothing, fgim_attack\n",
    "from data import prepare_data, non_pair_data_loader, get_cuda, pad_batch_seuqences, load_human_answer,\\\n",
    "    id2text_sentence, to_var, calc_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attributedict.collections import AttributeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = AttributeDict({'id_pad':0, 'id_unk':1, \"id_bos\" : 2, 'id_eos':3,'task':'dialect',\\\n",
    "                     'word_to_id_file': '', 'data_path' :'../../data/uae-eg/processed_files/', \\\n",
    "                     'word_dict_max_num' :5, 'batch_size' :128, 'max_sequence_length' :64, 'num_layers_AE':2,\n",
    "                     'transformer_model_size':256, 'transformer_ff_size' : 1024,'latent_size': 256, \\\n",
    "                     'word_dropout': 1.0, 'embedding_dropout':0.5,'learning_rate':0.001, 'label_size':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.id_bos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.if_load_from_checkpoint = True\n",
    "args.checkpoint_name = \"1588047455\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_log(ss):\n",
    "    now_time = time.strftime(\"[%Y-%m-%d %H:%M:%S]: \", time.localtime())\n",
    "    print(now_time + ss)\n",
    "    with open(args.log_file, 'a') as f:\n",
    "        f.write(now_time + str(ss) + '\\n')\n",
    "    return\n",
    "\n",
    "\n",
    "def add_output(ss):\n",
    "    with open(args.output_file, 'a') as f:\n",
    "        f.write(str(ss) + '\\n')\n",
    "    return\n",
    "\n",
    "\n",
    "def preparation():\n",
    "    # set model save path\n",
    "    if args.if_load_from_checkpoint:\n",
    "        timestamp = args.checkpoint_name\n",
    "    else:\n",
    "        timestamp = str(int(time.time()))\n",
    "        print(\"create new model save path: %s\" % timestamp)\n",
    "    args.current_save_path = 'save/%s/' % timestamp\n",
    "    args.log_file = args.current_save_path + time.strftime(\"log_%Y_%m_%d_%H_%M_%S.txt\", time.localtime())\n",
    "    args.output_file = args.current_save_path + time.strftime(\"output_%Y_%m_%d_%H_%M_%S.txt\", time.localtime())\n",
    "    print(\"create log file at path: %s\" % args.log_file)\n",
    "\n",
    "    if os.path.exists(args.current_save_path):\n",
    "        add_log(\"Load checkpoint model from Path: %s\" % args.current_save_path)\n",
    "    else:\n",
    "        os.makedirs(args.current_save_path)\n",
    "        add_log(\"Path: %s is created\" % args.current_save_path)\n",
    "\n",
    "    # set task type\n",
    "    if args.task == 'yelp':\n",
    "        args.data_path = '../../data/yelp/processed_files/'\n",
    "    elif args.task == 'amazon':\n",
    "        args.data_path = '../../data/amazon/processed_files/'\n",
    "    elif args.task == 'dialect':\n",
    "        args.data_path = '../../data/uae-eg/processed_files/'\n",
    "    elif args.task == 'imagecaption':\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError('Wrong task type!')\n",
    "\n",
    "    # prepare data\n",
    "    args.id_to_word, args.vocab_size, \\\n",
    "    args.train_file_list, args.train_label_list = prepare_data(\n",
    "        data_path=args.data_path, max_num=args.word_dict_max_num, task_type=args.task\n",
    "    )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create log file at path: save/1588047455/log_2020_04_28_19_30_34.txt\n",
      "[2020-04-28 19:30:34]: Load checkpoint model from Path: save/1588047455/\n",
      "prepare data ...\n",
      "Load word-dict with 41286 size and 5 max_num.\n"
     ]
    }
   ],
   "source": [
    "preparation()\n",
    "\n",
    "ae_model = get_cuda(make_model(d_vocab=args.vocab_size,\n",
    "                               N=args.num_layers_AE,\n",
    "                               d_model=args.transformer_model_size,\n",
    "                               latent_size=args.latent_size,\n",
    "                               d_ff=args.transformer_ff_size,\n",
    "))\n",
    "dis_model = get_cuda(Classifier(latent_size=args.latent_size, output_size=args.label_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_model.load_state_dict(torch.load(args.current_save_path + 'ae_model_params.pkl'))\n",
    "dis_model.load_state_dict(torch.load(args.current_save_path + 'dis_model_params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_iters(ae_model, dis_model):\n",
    "    eval_data_loader = non_pair_data_loader(\n",
    "        batch_size=1, id_bos=args.id_bos,\n",
    "        id_eos=args.id_eos, id_unk=args.id_unk,\n",
    "        max_sequence_length=args.max_sequence_length, vocab_size=args.vocab_size\n",
    "    )\n",
    "    eval_file_list = [\n",
    "        args.data_path + 'dialect_dev.uae',\n",
    "        args.data_path + 'dialect_dev.eg',\n",
    "    ]\n",
    "    eval_label_list = [\n",
    "        [0],\n",
    "        [1],\n",
    "    ]\n",
    "    eval_data_loader.create_batches(eval_file_list, eval_label_list, if_shuffle=False)\n",
    "#     gold_ans = load_human_answer(args.data_path)\n",
    "#     assert len(gold_ans) == eval_data_loader.num_batch\n",
    "\n",
    "    count = 0\n",
    "    add_log(\"Start eval process.\")\n",
    "    ae_model.eval()\n",
    "    dis_model.eval()\n",
    "    for it in range(eval_data_loader.num_batch):\n",
    "        batch_sentences, tensor_labels, \\\n",
    "        tensor_src, tensor_src_mask, tensor_tgt, tensor_tgt_y, \\\n",
    "        tensor_tgt_mask, tensor_ntokens = eval_data_loader.next_batch()\n",
    "\n",
    "        print(\"------------%d------------\" % it)\n",
    "        print(\"origin_input\",\"\\n label is \", tensor_labels)\n",
    "        print(id2text_sentence(tensor_tgt_y[0], args.id_to_word))\n",
    "\n",
    "        latent, out = ae_model.forward(tensor_src, tensor_tgt, tensor_src_mask, tensor_tgt_mask)\n",
    "        generator_text = ae_model.greedy_decode(latent,\n",
    "                                                max_len=args.max_sequence_length,\n",
    "                                                start_id=args.id_bos)\n",
    "#         print(\"------------------------\")\n",
    "#         print(\"autoencoder output:\")\n",
    "#         print(id2text_sentence(generator_text[0], args.id_to_word))\n",
    "\n",
    "        # Define target label\n",
    "        target = get_cuda(torch.tensor([[1.0]], dtype=torch.float))\n",
    "        if tensor_labels[0].item() > 0.5:\n",
    "            target = get_cuda(torch.tensor([[0.0]], dtype=torch.float))\n",
    "        print(\"target_labels\", target)\n",
    "\n",
    "        modify_text = fgim_attack(dis_model, latent, target, ae_model, args.max_sequence_length, args.id_bos,\n",
    "                                        id2text_sentence, args.id_to_word, '')\n",
    "        add_output(modify_text)\n",
    "        count += 1\n",
    "        if count >= 10:\n",
    "            break\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from ../../data/uae-eg/processed_files/dialect_dev.uae ../../data/uae-eg/processed_files/dialect_dev.eg !\n",
      "Create 59225 batches with 1 batch_size\n",
      "[2020-04-28 19:31:34]: Start eval process.\n",
      "------------0------------\n",
      "origin_input \n",
      " label is  tensor([[0.]], device='cuda:0')\n",
      "user الانضباط <UNK> hash num الف درهم <UNK> له <UNK> <UNK> <UNK> <UNK> في مباراه راس الخيمه\n",
      "target_labels tensor([[1.]], device='cuda:0')\n",
      "epsilon: 7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiyu94/py3.6/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| It  1 | dis model pred 0.0787 |\n",
      "hash وزاره <UNK> num مليون فقط <UNK> له الف <UNK> <UNK> <UNK> <UNK> <UNK> في راس الخيمه وفي اي حاجه سلمان العوده\n",
      "epsilon: 6.3\n",
      "| It  2 | dis model pred 0.7125 |\n",
      "hash المحكمه <UNK> num قال له الف <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> في راس الخيمه علشان <UNK> مباراه اكبر اتصالات\n",
      "epsilon: 5.67\n",
      "| It  3 | dis model pred 0.9597 |\n",
      "hash المحكمه <UNK> num قال له الف <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> في راس الخيمه علشان <UNK> مباراه اكبر اتصالات\n",
      "epsilon: 5.103\n",
      "| It  4 | dis model pred 0.9713 |\n",
      "hash المحكمه <UNK> num قال له الف <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> في راس الخيمه علشان <UNK> مباراه اكبر اتصالات\n",
      "epsilon: 4.5927\n",
      "| It  5 | dis model pred 0.9767 |\n",
      "hash المحكمه <UNK> num قال له الف <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> في راس الخيمه علشان <UNK> مباراه اكبر اتصالات\n",
      "epsilon: 8.0\n",
      "| It  1 | dis model pred 0.0787 |\n",
      "hash وزاره <UNK> num مليون فقط <UNK> له الف <UNK> <UNK> <UNK> <UNK> <UNK> في راس الخيمه وفي اي حاجه سلمان العوده\n",
      "epsilon: 7.2\n",
      "| It  2 | dis model pred 0.8126 |\n",
      "hash المحكمه <UNK> num الف hash له <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> في راس الخيمه عشان مباراه اكبر hash\n",
      "epsilon: 6.48\n",
      "| It  3 | dis model pred 0.9721 |\n",
      "hash المحكمه <UNK> num قال هناك الف <UNK> <UNK> <UNK> <UNK> <UNK> له <UNK> في راس الخيمه انك عايزين نطلع hash\n",
      "epsilon: 5.832000000000001\n",
      "| It  4 | dis model pred 0.9783 |\n",
      "hash المحكمه <UNK> num قال هناك الف <UNK> <UNK> <UNK> <UNK> <UNK> له <UNK> في راس الخيمه انك عايزين نطلع hash\n",
      "epsilon: 5.248800000000001\n",
      "| It  5 | dis model pred 0.9814 |\n",
      "hash المحكمه <UNK> num قال هناك الف <UNK> <UNK> <UNK> <UNK> <UNK> له <UNK> في راس الخيمه انك عايزين نطلع فيه مطار\n",
      "------------1------------\n",
      "origin_input \n",
      " label is  tensor([[0.]], device='cuda:0')\n",
      "بعض البشر يغرك بصدق الاحساس num وبعض البشر hash وقلبه يحبك\n",
      "target_labels tensor([[1.]], device='cuda:0')\n",
      "epsilon: 7.0\n",
      "| It  1 | dis model pred 0.1535 |\n",
      "ان البشر تسقط hash النعيم بصدق num ولكن هناك اشخاصا <UNK> راشق فيه وهو وجه الشخص اليكم\n",
      "epsilon: 6.3\n",
      "| It  2 | dis model pred 0.6270 |\n",
      "ان البشر تسقط hash النعيم بصدق num ولكن هناك اشخاصا <UNK> راشق فيه وهو وجه الشخص اليكم\n",
      "epsilon: 5.67\n",
      "| It  3 | dis model pred 0.7060 |\n",
      "ان البشر تسقط hash النعيم بصدق num ولكن هناك موقف تحرر المحبه كان ربك دائما فانه يتالم\n",
      "epsilon: 5.103\n",
      "| It  4 | dis model pred 0.7514 |\n",
      "ان البشر تسقط hash النعيم بصدق num ولكن هناك موقف تحرر المحبه كان ربك دائما فانه يتالم\n",
      "epsilon: 4.5927\n",
      "| It  5 | dis model pred 0.7811 |\n",
      "ان البشر تسقط hash النعيم بصدق num ولكن هناك موقف تحرر المحبه كان ربك دائما فانه يتالم\n",
      "epsilon: 8.0\n",
      "| It  1 | dis model pred 0.1535 |\n",
      "عندما يحبك بصدق الناس وبعض البشر hash num ولكن يزرع الله كثيرا الدور ل بہ حكمه وقلبه num\n",
      "epsilon: 7.2\n",
      "| It  2 | dis model pred 0.6436 |\n",
      "عندما يحبك بصدق الناس وبعض البشر hash num ولكن من يزرع المطر القران كده بك وهو علي التقدم\n",
      "epsilon: 6.48\n",
      "| It  3 | dis model pred 0.7183 |\n",
      "ان البشر تسقط hash النعيم بصدق num ولكن هناك موقف تحرر المحبه كان ربك دائما فانه يتالم\n",
      "epsilon: 5.832000000000001\n",
      "| It  4 | dis model pred 0.7664 |\n",
      "ان البشر تسقط hash النعيم بصدق num ولكن هناك موقف تحرر المحبه كان ربك دائما فانه يتالم\n",
      "epsilon: 5.248800000000001\n",
      "| It  5 | dis model pred 0.7942 |\n",
      "ان البشر تسقط hash النعيم بصدق num ولكن هناك موقف تحرر المحبه كان ربك دائما فانه يتالم\n",
      "------------2------------\n",
      "origin_input \n",
      " label is  tensor([[0.]], device='cuda:0')\n",
      "الي <UNK> لفتره <UNK> <UNK> الناس والناس يدرون فقط <UNK>\n",
      "target_labels tensor([[1.]], device='cuda:0')\n",
      "epsilon: 7.0\n",
      "| It  1 | dis model pred 0.4200 |\n",
      "<UNK> الي <UNK> <UNK> <UNK> بعض الناس سريعا num تحسب انهم <UNK> <UNK> كده واهلك انفسنا لكن <UNK>\n",
      "epsilon: 6.3\n",
      "| It  2 | dis model pred 0.6376 |\n",
      "<UNK> الي <UNK> <UNK> <UNK> بعض الناس سريعا بس من كاتب <UNK> <UNK> الاعمال الاوروبيه num\n",
      "epsilon: 5.67\n",
      "| It  3 | dis model pred 0.9207 |\n",
      "<UNK> الي <UNK> <UNK> <UNK> بعض الناس بتاعك بس تغفر علشان <UNK> سنه بورسعيد <UNK> ف التجربه\n",
      "epsilon: 5.103\n",
      "| It  4 | dis model pred 0.9449 |\n",
      "<UNK> الي <UNK> <UNK> <UNK> الناس مو <UNK> <UNK> <UNK> ادني واحده بس عن <UNK> الزباله\n",
      "epsilon: 4.5927\n",
      "| It  5 | dis model pred 0.9556 |\n",
      "<UNK> الي <UNK> <UNK> <UNK> والناس <UNK> الناس ميديا انه تم <UNK> <UNK> سجاير بس اسال نفسك\n",
      "epsilon: 8.0\n",
      "| It  1 | dis model pred 0.4200 |\n",
      "<UNK> الي <UNK> <UNK> <UNK> بعض الناس سريعا num تحسب انهم <UNK> <UNK> كده واهلك انفسنا لكن <UNK>\n",
      "epsilon: 7.2\n",
      "| It  2 | dis model pred 0.6509 |\n",
      "<UNK> الي <UNK> <UNK> <UNK> بعض الناس سريعا بس من كاتب <UNK> <UNK> الاعمال الاوروبيه num\n",
      "epsilon: 6.48\n",
      "| It  3 | dis model pred 0.9407 |\n",
      "<UNK> الي <UNK> <UNK> <UNK> بعض الناس سريعا بس من كاتب <UNK> <UNK> الاعمال دي لكن تعبانه hash\n",
      "epsilon: 5.832000000000001\n",
      "| It  4 | dis model pred 0.9611 |\n",
      "<UNK> الي <UNK> <UNK> <UNK> ليه الناس ومش تحبہ <UNK> الوجع لان واحده بس <UNK> الذين مؤمنين بقي\n",
      "epsilon: 5.248800000000001\n",
      "| It  5 | dis model pred 0.9688 |\n",
      "<UNK> الي <UNK> <UNK> <UNK> ليه الناس ومش بتعمله كام <UNK> و <UNK> الحسين صعب طبعا\n",
      "------------3------------\n",
      "origin_input \n",
      " label is  tensor([[0.]], device='cuda:0')\n",
      "<UNK> بعدين <UNK> هذه شيم <UNK>\n",
      "target_labels tensor([[1.]], device='cuda:0')\n",
      "epsilon: 7.0\n",
      "| It  1 | dis model pred 0.5104 |\n",
      "<UNK> <UNK> بعدين <UNK> هذه الزلات <UNK> <UNK> ولو اخاف <UNK> <UNK> شئ انما <UNK> <UNK> بحياتي num\n",
      "epsilon: 6.3\n",
      "| It  2 | dis model pred 0.6415 |\n",
      "<UNK> <UNK> بعدين <UNK> هذه الزلات <UNK> ولو فيه مؤتمر الندم <UNK> num سواء الرحمن <UNK> عن <UNK> num\n",
      "epsilon: 5.67\n",
      "| It  3 | dis model pred 0.8239 |\n",
      "<UNK> <UNK> بعدين <UNK> هذه الزلات <UNK> ولو فيه مؤتمر الندم <UNK> num feeling sad\n",
      "epsilon: 5.103\n",
      "| It  4 | dis model pred 0.8674 |\n",
      "<UNK> <UNK> بعدين <UNK> هذه الزلات <UNK> ولو فيه مؤتمر الندم <UNK> num feeling sad\n",
      "epsilon: 4.5927\n",
      "| It  5 | dis model pred 0.8882 |\n",
      "<UNK> <UNK> بعدين <UNK> هذه الزلات <UNK> ولو فيه مؤتمر لجان واحده <UNK> num اخيرا <UNK> شر\n",
      "epsilon: 8.0\n",
      "| It  1 | dis model pred 0.5104 |\n",
      "<UNK> <UNK> بعدين <UNK> هذه الزلات <UNK> <UNK> ولو اخاف <UNK> شئ num مصطفي البصري دا\n",
      "epsilon: 7.2\n",
      "| It  2 | dis model pred 0.6494 |\n",
      "<UNK> <UNK> بعدين <UNK> هذه الزلات <UNK> ولو فيه مؤتمر الندم <UNK> num سواء الرحمن <UNK> عن <UNK> num\n",
      "epsilon: 6.48\n",
      "| It  3 | dis model pred 0.8203 |\n",
      "<UNK> <UNK> بعدين <UNK> هذه الزلات <UNK> ولو فيه مؤتمر الندم <UNK> num feeling sad\n",
      "epsilon: 5.832000000000001\n",
      "| It  4 | dis model pred 0.8688 |\n",
      "<UNK> <UNK> بعدين <UNK> هذه الزلات <UNK> ولو فيه مؤتمر الندم <UNK> num feeling sad\n",
      "epsilon: 5.248800000000001\n",
      "| It  5 | dis model pred 0.8940 |\n",
      "<UNK> <UNK> بعدين <UNK> هذه الزلات <UNK> ولو فيه مؤتمر لجان واحده <UNK> num اخيرا <UNK> شر\n",
      "------------4------------\n",
      "origin_input \n",
      " label is  tensor([[0.]], device='cuda:0')\n",
      "اقم الصلاه <UNK> الشمس الي غسق الليل وقران الفجر ان قران الفجر كان مشهودا ٧٨ سوره الاسراء hash\n",
      "target_labels tensor([[1.]], device='cuda:0')\n",
      "epsilon: 7.0\n",
      "| It  1 | dis model pred 0.3724 |\n",
      "اقم الصلاه <UNK> الشمس الي بطون اثنين من حفظها الله الاهلي كان الفجر قران الغفله ٢ سوره الاسراء hash\n",
      "epsilon: 6.3\n",
      "| It  2 | dis model pred 0.4896 |\n",
      "اقم الصلاه <UNK> الشمس الي بطون اثنين من يتبدل لكم الليل والنهار لو كان قران الفجر بخير ٧٧ سوره الاسراء hash\n",
      "epsilon: 5.67\n",
      "| It  3 | dis model pred 0.5903 |\n",
      "اقم الصلاه <UNK> الشمس الي بطون اثنين من يتبدل لكم الليل والنهار لو كان قران الفجر الفوز حكيما واصيلا hash\n",
      "epsilon: 5.103\n",
      "| It  4 | dis model pred 0.6471 |\n",
      "اقم الصلاه <UNK> الشمس الي بطون اثنين من يتبدل لكم الليل والنهار لو كان قران الفجر الفوز حكيما واصيلا hash\n",
      "epsilon: 4.5927\n",
      "| It  5 | dis model pred 0.6949 |\n",
      "اقم الصلاه <UNK> الشمس الي بطون اثنين من يتبدل لكم الليل والنهار لو كان قران الفجر شئ مبينا ٥٨ سوره الاسراء hash\n",
      "epsilon: 8.0\n",
      "| It  1 | dis model pred 0.3724 |\n",
      "اقم الصلاه <UNK> الشمس الي بطون اثنين من حفظها الله الاهلي كان الفجر قران الغفله ٢ سوره الاسراء hash\n",
      "epsilon: 7.2\n",
      "| It  2 | dis model pred 0.5060 |\n",
      "اقم الصلاه <UNK> الشمس الي بطون اثنين من يتبدل لكم الليل والنهار لو كان قران الفجر الفوز حكيما واصيلا hash\n",
      "epsilon: 6.48\n",
      "| It  3 | dis model pred 0.6165 |\n",
      "اقم الصلاه <UNK> الشمس الي بطون اثنين من يتبدل لكم الليل والنهار لو كان قران الفجر الفوز حكيما واصيلا hash\n",
      "epsilon: 5.832000000000001\n",
      "| It  4 | dis model pred 0.6705 |\n",
      "اقم الصلاه <UNK> الشمس الي بطون اثنين من يتبدل لكم الليل والنهار لو كان قران الفجر شئ مبينا ٥٨ سوره الاسراء hash\n",
      "epsilon: 5.248800000000001\n",
      "| It  5 | dis model pred 0.7203 |\n",
      "اقم الصلاه <UNK> الشمس الي بطون اثنين من يتبدل لكم الليل والنهار لو كان قران الفجر شئ مبينا ٥٨ سوره الاسراء hash\n",
      "------------5------------\n",
      "origin_input \n",
      " label is  tensor([[0.]], device='cuda:0')\n",
      "اجمل النساء امراه سهرت و تعبت و ربت و اعطت دون مقابل num تدعي امي\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_labels tensor([[1.]], device='cuda:0')\n",
      "epsilon: 7.0\n",
      "| It  1 | dis model pred 0.6406 |\n",
      "اكتر قلبي بيجيب احلام و تعبت و <UNK> عانقت نفسك num و عندكو تعلم جرح ام غيرها num شكلها لوحده\n",
      "epsilon: 6.3\n",
      "| It  2 | dis model pred 0.9626 |\n",
      "اكتر قلبي بيجيب احلام و تعبت و <UNK> عانقت نفسك num و عندكو تعلم جرح ام غيرها num شكلها لوحده\n",
      "epsilon: 5.67\n",
      "| It  3 | dis model pred 0.9714 |\n",
      "اكتر قلبي بيجيب احلام و تعبت و <UNK> عانقت نفسك num و عندكو تعلم جرح ام غيرها num شكلها لوحده\n",
      "epsilon: 5.103\n",
      "| It  4 | dis model pred 0.9755 |\n",
      "اكتر قلبي بيجيب احلام و تعبت و <UNK> عانقت نفسك num و عندكو تعلم جرح ام غيرها num شكلها لوحده\n",
      "epsilon: 4.5927\n",
      "| It  5 | dis model pred 0.9782 |\n",
      "اكتر قلبي بيجيب احلام و تعبت و <UNK> عانقت نفسك num و عندكو تعلم جرح ام غيرها num شكلها لوحده\n",
      "epsilon: 8.0\n",
      "| It  1 | dis model pred 0.6406 |\n",
      "اكتر قلبي بيجيب احلام و تعبت و <UNK> عانقت نفسك num و عندكو تعلم جرح ام غيرها num شكلها لوحده\n",
      "epsilon: 7.2\n",
      "| It  2 | dis model pred 0.9731 |\n",
      "اكتر قلبي بيجيب احلام و تعبت و <UNK> عانقت نفسك num و عندكو تعلم جرح ام غيرها num شكلها لوحده\n",
      "epsilon: 6.48\n",
      "| It  3 | dis model pred 0.9774 |\n",
      "اكتر قلبي بيجيب احلام و تعبت و <UNK> عانقت نفسك num و عندكو تعلم جرح ام غيرها num شكلها لوحده\n",
      "epsilon: 5.832000000000001\n",
      "| It  4 | dis model pred 0.9802 |\n",
      "اكتر قلبي بيجيب احلام و تعبت و <UNK> ندامه و عايز num من انها قايله ليها فكره الورد <UNK> بالمره\n",
      "epsilon: 5.248800000000001\n",
      "| It  5 | dis model pred 0.9821 |\n",
      "اكتر قلبي بيجيب احلام و تعبت و <UNK> ندامه و عايز num من انها قايله ليها فكره الورد <UNK> بالمره\n",
      "------------6------------\n",
      "origin_input \n",
      " label is  tensor([[0.]], device='cuda:0')\n",
      "خيره الله تبقي فوق كل وجع ل يهمس لك قلبك كفاك الله شيئا تحبه كي لا يمسك ضر كفاك كي ينبت لك فرحا اكبر واجمل\n",
      "target_labels tensor([[1.]], device='cuda:0')\n",
      "epsilon: 7.0\n",
      "| It  1 | dis model pred 0.0277 |\n",
      "لعل الله تصبح كل شيء جميلا لا قسي قلبك عني لك قلبك كي لن يخلق لك تحبه كي لن ياخذه الالم هيبقي اكثر منك عسي ان تعود\n",
      "epsilon: 6.3\n",
      "| It  2 | dis model pred 0.1693 |\n",
      "لعل الله تصبح كل شيء جميلا علي قلبك قسي قلبك كي لا تحبه وقل لك لن ادعو لك الوقت في والارض ان اعشق العسر وحزنك نحو ربك\n",
      "epsilon: 5.67\n",
      "| It  3 | dis model pred 0.5217 |\n",
      "لعل الله تصبح كل شيء جميلا علي قلبك بغير قادر اكتبه يتبعه <UNK> لك كي تحبه كي لن يخلق لك شيئا عند قصد num عسي الله ان معااك\n",
      "epsilon: 5.103\n",
      "| It  4 | dis model pred 0.6805 |\n",
      "لعل الله تصبح كل شيء جميلا علي قلبك بغير قادر اكتبه كي لك عسي ان يمسك ضرر لن يخلق لك نحو قلبك كي تنتهي والكثير اليك\n",
      "epsilon: 4.5927\n",
      "| It  5 | dis model pred 0.7460 |\n",
      "لعل الله تصبح كل شيء جميلا علي قلبك بغير قادر اكتبه لك كي لا يخلق لك بصدق كفاك الله وكيف اعرف ان جدا نحو الحلم والارض\n",
      "epsilon: 8.0\n",
      "| It  1 | dis model pred 0.0277 |\n",
      "لعل الله تصبح كل شيء جميلا علي قلبك قسي قلبك كي لا تحبه وقل لك لن يضل عند يقين ان اول لك كي احتاجك لنفسي\n",
      "epsilon: 7.2\n",
      "| It  2 | dis model pred 0.2055 |\n",
      "لعل الله تصبح كل شيء جميلا علي قلبك قسي قلبك كي لا تحبه وقل لك لن ادعو لك ان عند التقدم نحو ربك كي احتاجك\n",
      "epsilon: 6.48\n",
      "| It  3 | dis model pred 0.5961 |\n",
      "لعل الله تصبح كل شيء جميلا علي قلبك بغير قادر اكتبه يتبعه <UNK> لك كي تحبه كي لن يخلق لك شيئا عند ان يمسك هذا التوكل d\n",
      "epsilon: 5.832000000000001\n",
      "| It  4 | dis model pred 0.7195 |\n",
      "لعل الله تصبح كل شيء جميلا علي قلبك بغير قادر اكتبه لك كي لا يخلق لك بصدق كفاك الله وكيف اعرف ان جدا نحو الحلم والارض\n",
      "epsilon: 5.248800000000001\n",
      "| It  5 | dis model pred 0.7812 |\n",
      "لعل الله تصبح كل شيء جميلا علي قلبك بغير قادر اكتبه لك كي لا يخلق لك بصدق نحو السماء والارض لكان ان المواطن لن تخيب عنك\n",
      "------------7------------\n",
      "origin_input \n",
      " label is  tensor([[0.]], device='cuda:0')\n",
      "الي بريطانيا باذن الله استودعكم الله\n",
      "target_labels tensor([[1.]], device='cuda:0')\n",
      "epsilon: 7.0\n",
      "| It  1 | dis model pred 0.1366 |\n",
      "في ال hash لانني <UNK> وانت يعافيك الله و اليوم بناء دوله <UNK>\n",
      "epsilon: 6.3\n",
      "| It  2 | dis model pred 0.5893 |\n",
      "في ال hash بحيث تقول الله معك الذين كتمت من النهار وكان الله تعالي <UNK> المصريه\n",
      "epsilon: 5.67\n",
      "| It  3 | dis model pred 0.7621 |\n",
      "في ال hash بعدين يسالوني الله من ايديك وانما <UNK> ليس زي النبي صلي الله عليه وسلم\n",
      "epsilon: 5.103\n",
      "| It  4 | dis model pred 0.9160 |\n",
      "في ال hash يالهوي جدا اي عمل السعااده سبحان الله للاسف الثوره\n",
      "epsilon: 4.5927\n",
      "| It  5 | dis model pred 0.9329 |\n",
      "في ال hash يالهوي جدا اي عمل البرسا كلنا بعد ما سلاما\n",
      "epsilon: 8.0\n",
      "| It  1 | dis model pred 0.1366 |\n",
      "في ال hash لانني <UNK> وانت يعافيك الله و اليوم بناء دوله <UNK>\n",
      "epsilon: 7.2\n",
      "| It  2 | dis model pred 0.5818 |\n",
      "في ال hash بحيث تقول الله معك الذين كتمت من النهار وكان الله تعالي <UNK> المصريه\n",
      "epsilon: 6.48\n",
      "| It  3 | dis model pred 0.7853 |\n",
      "في ال hash يالهوي لكن الله اسالك من الخبره ماشاء الله فقط\n",
      "epsilon: 5.832000000000001\n",
      "| It  4 | dis model pred 0.9414 |\n",
      "في ال hash يالهوي لكن الله اسالك من الخبره ماشاء الله فقط\n",
      "epsilon: 5.248800000000001\n",
      "| It  5 | dis model pred 0.9582 |\n",
      "في ال hash يالهوي لكن الله اسالك من النتيجه رجا دا لو <UNK>\n",
      "------------8------------\n",
      "origin_input \n",
      " label is  tensor([[0.]], device='cuda:0')\n",
      "الابتسامہ كانت صدقه ف اصبحت في مجتمعنا شيء يثير الشك داخل الاخرين num\n",
      "target_labels tensor([[1.]], device='cuda:0')\n",
      "epsilon: 7.0\n",
      "| It  1 | dis model pred 0.3009 |\n",
      "كانت الفضفضه ب قلبي اصبحت في اي حركه الشك والاستقرار وهو حاجه داخل رحله التفكير كثيرا num hash\n",
      "epsilon: 6.3\n",
      "| It  2 | dis model pred 0.7407 |\n",
      "كانت الفضفضه ب قلبي اصبحت في اي حركه الشك والاستقرار وهو حاجه داخل الندم مش قليل num hash\n",
      "epsilon: 5.67\n",
      "| It  3 | dis model pred 0.8264 |\n",
      "كانت الفضفضه ب قلبي اصبحت في اي حركه الشك والاستقرار وهو حاجه داخل الندم مش قليل num hash\n",
      "epsilon: 5.103\n",
      "| It  4 | dis model pred 0.8599 |\n",
      "كانت الفضفضه ب قلبي اصبحت في اي حركه الشك الناجح لكن تكون مريض نتكلم بنفسك num hash\n",
      "epsilon: 4.5927\n",
      "| It  5 | dis model pred 0.8818 |\n",
      "كانت الفضفضه ب قلبي اصبحت في اي حركه الشك الناجح لكن تكون مريض نتكلم بنفسك num hash\n",
      "epsilon: 8.0\n",
      "| It  1 | dis model pred 0.3009 |\n",
      "كانت الفضفضه ب قلبي اصبحت في اي حركه الشك والاستقرار وهو حاجه داخل رحله التفكير كثيرا num hash\n",
      "epsilon: 7.2\n",
      "| It  2 | dis model pred 0.7802 |\n",
      "كانت الفضفضه ب قلبي اصبحت في اي حركه الشك الناجح لكن تكون مريض نتكلم بنفسك num hash\n",
      "epsilon: 6.48\n",
      "| It  3 | dis model pred 0.8488 |\n",
      "كانت الفضفضه ب قلبي اصبحت في اي حركه الشك الناجح لكن تكون مريض نتكلم بنفسك num hash\n",
      "epsilon: 5.832000000000001\n",
      "| It  4 | dis model pred 0.8790 |\n",
      "كانت الفضفضه ب قلبي اصبحت في اي حركه الشك الناجح لكن تكون مريض نتكلم بنفسك num hash\n",
      "epsilon: 5.248800000000001\n",
      "| It  5 | dis model pred 0.8977 |\n",
      "كانت الفضفضه ب قلبي اصبحت في اي حركه الشك الناجح لكن تكون مريض نتكلم بنفسك num hash\n",
      "------------9------------\n",
      "origin_input \n",
      " label is  tensor([[0.]], device='cuda:0')\n",
      "اللهم num في يوم الجمعه اجمع قلوبنا علي طاعتك و اجمع نفوسنا علي خشيتك و اجمع ارواحنا في جنتك\n",
      "target_labels tensor([[1.]], device='cuda:0')\n",
      "epsilon: 7.0\n",
      "| It  1 | dis model pred 0.1117 |\n",
      "اللهم في يوم num اجمع قلوبنا علي طاعتك و نفوسنا علي خشيتك و اجمع ارواحنا في جنتك و اجمع <UNK> القدر\n",
      "epsilon: 6.3\n",
      "| It  2 | dis model pred 0.8263 |\n",
      "اللهم في يوم num اجمع قلوبنا علي طاعتك و نفوسنا علي خشيتك و اجمع ارواحنا في جنتك و اجمع <UNK> القدر\n",
      "epsilon: 5.67\n",
      "| It  3 | dis model pred 0.9695 |\n",
      "اللهم في يوم num اجمع قلوبنا علي طاعتك و نفوسنا علي خشيتك و اجمع ارواحنا في جنتك و اجمع <UNK> زي النسيان\n",
      "epsilon: 5.103\n",
      "| It  4 | dis model pred 0.9767 |\n",
      "اللهم في يوم num اجمع قلوبنا علي طاعتك و نفوسنا علي خشيتك و اجمع ارواحنا في جنتك و اجمع <UNK> زي نفسه\n",
      "epsilon: 4.5927\n",
      "| It  5 | dis model pred 0.9807 |\n",
      "اللهم في يوم num اجمع قلوبنا علي طاعتك و نفوسنا علي خشيتك و اجمع ارواحنا في جنتك <UNK> و اجمع منا علي يقين\n",
      "epsilon: 8.0\n",
      "| It  1 | dis model pred 0.1117 |\n",
      "اللهم في يوم num اجمع قلوبنا علي طاعتك و نفوسنا علي خشيتك و اجمع ارواحنا في جنتك و اجمع <UNK> القدر\n",
      "epsilon: 7.2\n",
      "| It  2 | dis model pred 0.9009 |\n",
      "اللهم في يوم num اجمع قلوبنا علي طاعتك و نفوسنا علي خشيتك و اجمع ارواحنا في جنتك و اجمع <UNK> القدر\n",
      "epsilon: 6.48\n",
      "| It  3 | dis model pred 0.9674 |\n",
      "اللهم في يوم num اجمع قلوبنا علي طاعتك و نفوسنا علي خشيتك و اجمع ارواحنا في جنتك و اجمع <UNK> زي نفسه\n",
      "epsilon: 5.832000000000001\n",
      "| It  4 | dis model pred 0.9761 |\n",
      "اللهم في يوم num اجمع قلوبنا علي طاعتك و نفوسنا علي خشيتك و اجمع ارواحنا في جنتك و اجمع <UNK> زي نفسه\n",
      "epsilon: 5.248800000000001\n",
      "| It  5 | dis model pred 0.9800 |\n",
      "اللهم في يوم num اجمع قلوبنا علي طاعتك و نفوسنا علي خشيتك و اجمع ارواحنا في جنتك و اجمع <UNK> زي نفسه\n"
     ]
    }
   ],
   "source": [
    "eval_iters(ae_model, dis_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderDecoder(\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (sublayer): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (sublayer): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (sublayer): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (w_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (sublayer): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm()\n",
      "  )\n",
      "  (src_embed): Sequential(\n",
      "    (0): Embeddings(\n",
      "      (lut): Embedding(41286, 256)\n",
      "    )\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (tgt_embed): Sequential(\n",
      "    (0): Embeddings(\n",
      "      (lut): Embedding(41286, 256)\n",
      "    )\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Generator(\n",
      "    (proj): Linear(in_features=256, out_features=41286, bias=True)\n",
      "  )\n",
      "  (position_layer): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(ae_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
